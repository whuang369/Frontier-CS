name: Weekly Batch Evaluation

on:
  schedule:
    # Run every Sunday at 00:00 UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      problems:
        description: 'Comma-separated problem IDs (leave empty for all)'
        required: false
        default: ''
      max_concurrent:
        description: 'Max concurrent evaluations'
        required: false
        default: '4'

env:
  SKYPILOT_CLOUD: gcp

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        run: |
          uv sync
          uv pip install skypilot[gcp]

      - name: Set up GCP credentials
        run: |
          echo '${{ secrets.GCP_CREDENTIALS }}' > /tmp/gcp-key.json
          # Set as application default credentials for SkyPilot
          mkdir -p ~/.config/gcloud
          cp /tmp/gcp-key.json ~/.config/gcloud/application_default_credentials.json
          # For service account, also activate with gcloud
          if grep -q '"type": "service_account"' /tmp/gcp-key.json; then
            gcloud auth activate-service-account --key-file=/tmp/gcp-key.json
          fi
          gcloud config set project ${{ secrets.GCP_PROJECT_ID }}
        env:
          GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-key.json

      - name: Configure SkyPilot
        run: |
          mkdir -p ~/.sky
          # SkyPilot uses GCP project from gcloud config or ADC, not from sky config
          sky check gcp || true

      - name: Generate pairs file (incremental)
        run: |
          # Get list of already evaluated pairs from results
          uv run python -c "
          import json
          from pathlib import Path

          # Read existing results
          results_dir = Path('results/batch')
          evaluated = set()
          if results_dir.exists():
              for f in results_dir.glob('*.json'):
                  try:
                      data = json.loads(f.read_text())
                      if data.get('status') == 'success':
                          evaluated.add(data.get('pair_id', ''))
                  except:
                      pass

          # Get list of problem names
          problems_dir = Path('research/problems')
          problem_names = set()
          if problems_dir.exists():
              for p in problems_dir.iterdir():
                  if p.is_dir():
                      problem_names.add(p.name)

          # Read all possible pairs from solutions/
          solutions_dir = Path('solutions')
          pairs = []
          if solutions_dir.exists():
              for sol_dir in solutions_dir.iterdir():
                  if sol_dir.is_dir() and (sol_dir / 'solve.sh').exists():
                      # Extract problem from solution name: {model}_{problem}[_{variant}]
                      name = sol_dir.name
                      # Find the longest matching problem name
                      parts = name.split('_')
                      best_problem = None
                      for i in range(1, len(parts) + 1):
                          candidate = '_'.join(parts[1:i])
                          if candidate in problem_names:
                              best_problem = candidate
                      if best_problem:
                          pairs.append(f'{name}:{best_problem}')

          # Filter out already evaluated
          new_pairs = [p for p in pairs if p not in evaluated]

          # Write pairs file
          Path('pairs_to_eval.txt').write_text('\n'.join(new_pairs))
          print(f'Total pairs: {len(pairs)}, New pairs to evaluate: {len(new_pairs)}')
          "

      - name: Run batch evaluation
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Skip if no pairs to evaluate
          if [ ! -s pairs_to_eval.txt ]; then
            echo "No new pairs to evaluate"
            exit 0
          fi

          MAX_CONCURRENT="${{ github.event.inputs.max_concurrent || '4' }}"

          uv run frontier-eval batch \
            --pairs-file pairs_to_eval.txt \
            --skypilot \
            --max-concurrent $MAX_CONCURRENT \
            --results-dir results/batch

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results-${{ github.run_id }}
          path: results/
          retention-days: 90

      - name: Commit results (optional)
        if: github.event_name == 'schedule'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add results/
          git diff --staged --quiet || git commit -m "chore: update evaluation results [skip ci]"
          git push
